# -*- coding: utf-8 -*-
"""PythonAssessment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XtCSXQbq0M0cYNaoDF6YQlDjLaflCIoK
"""

import pyspark
from pyspark.sql import SparkSession
spark = SparkSession.builder.getOrCreate()

"""Below is the function to change all the column headers to snake_case.

Note: I have only considered space,/ and - after looking one or two files.



"""

from pyspark.sql.functions import col
def rename_cols(rename_df):
    for column in rename_df.columns:
        new_column = column.replace(' ','_').replace('/','_').replace('-','_')
        rename_df = rename_df.withColumnRenamed(column, new_column)
    return rename_df

"""Below is the main code to read the csv files from datastore that are related to the theme "Hospitals" and load them to storage path.

**NOTE :**

1. As I dont have any work environment of Azure to load the files, I have saved all the files from data store to my local path.

2. Scheduling the job is pending. As I dont have Azure databricks environment, I am not able to schedule it. This should be a very simply task in Databricks to create a job and schedule it daily. This code should take care of loading only those files that have been modified.
I have considered the column modifiedDate to track the changes of the file.


"""

import gettext
import json
import requests
import pandas as pd
import os

api_url = f'https://data.cms.gov/provider-data/api/1/metastore/schemas/dataset/items'

getjsondata = requests.get(api_url)

response_json = json.loads(getjsondata.text)

#Looping through all the json records
for todoFile in response_json:

  #Looking only for the theme Hospitals
  if str(todoFile["theme"]) == "['Hospitals']":

    #Extracting only the filename from the URL
    fileName = os.path.basename(todoFile["distribution"][0]["downloadURL"])


    #Extracting ModifiedDate column to append to filename and also to identity if the file has already loaded or not
    modifiedDate = todoFile["modified"]

    #updating the modifiedDate format to YYYYMMDD
    modifiedDateFormatted = modifiedDate.replace("-", "")

    #Generating the fileName for saving YYYYMMDD_FileName.csv
    fileNameFinal =  modifiedDateFormatted + "_" + fileName

    #Reading the csv file using pandas dataframe
    initialDf = pd.read_csv(todoFile["distribution"][0]["downloadURL"])

    #Checking if the file already exists or loaded. We will only load the file if it is not there in our folder
    if not os.path.exists("/Users/vinay/CMSData/" + fileNameFinal):
      #Note : have converted panda dataframe to spark dataframe for renaming columns. I did this quick turnaround as I have already spent most of the time on this.
      spark_df = spark.createDataFrame(initialDf)

      #Calling the function to change all the column headers to snake_case.
      spark_df1 = rename_cols(spark_df)

      #Finally writihng the file to the storage location.
      spark_df1.write.option("header",True).mode("overwrite").csv("/Users/vinay/CMSData/" + fileNameFinal)
    else:
      print("File Already exists")

#Took one randome file to read/verify the contents after loading
spark.read.csv("/Users/vinay/CMSData/20241010_Timely_and_Effective_Care-State.csv",header = True).show()

#Took one more randome file to read/verify the contents after loading
spark.read.csv("/Users/vinay/CMSData/20241018_HCAHPS-Hospital.csv",header = True).show()

#Took one more randome file to read/verify the contents after loading
spark.read.csv("/Users/vinay/PythAssessment/20241010_Complications_and_Deaths-Hospital.csv",header = True).show()